{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model predicting thermal sensation using given database\n",
    "\n",
    "Link to database: https://github.com/CenterForTheBuiltEnvironment/ashrae-db-II.git\n",
    "\n",
    "Packages: \n",
    "1. pandas\n",
    "2. scipy \n",
    "3. math - no need \n",
    "4. numpy\n",
    "5. scikit\n",
    "6. tensorflow and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe \n",
    "\n",
    "Using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pathlib\n",
    "\n",
    "#create dataframe from data csv file as df\n",
    "df = pd.read_csv(\"db_measurements_v2.1.0.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values\n",
    "\n",
    "Given the fact that the dataset consists of a collection of different studies, each of which take into consideration varied parameters, the following code calculates the amount of NaN values on each column of the dataframe. The aim here is to find the parameters most commonly used in studies to create a final dataset as consistent as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to find percentage of NaNs per column, types it in txt file\n",
    "\n",
    "#create percentages\n",
    "size = df['index'].size + 1\n",
    "nan_array = df.isnull().sum() / size * 100 #creates a series of the percentages\n",
    "\n",
    "#store in file\n",
    "nan_array_string = [\"%.2f\" % i for i in nan_array] #turns percentages into strings\n",
    "\n",
    "data = {df.columns[col]: nan_array_string[col] for col in range(nan_array.size)} #makes dict and dataframe\n",
    "nan_df = pd.DataFrame(data.items())\n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data.csv' #stores in file\n",
    "nan_df.to_csv(path, header=None, index = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sorting the dataset's columns by their amount of NaN values can allow for an easy selection of columns to keep for the analysis and later prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort through nan series and cut all percentages above 50%\n",
    "\n",
    "nan_array_sorted = nan_array.sort_values(ascending=True) #sorts throught the series \n",
    "nan_array_sorted = nan_array_sorted[nan_array_sorted<50.0] #only keeps columns with below 50% NaN cells \n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data_sorted.csv' #stores file for future use\n",
    "nan_array_sorted.to_csv(path, header = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the file produced and relevant bibliography and keeping in mind that the ultimate goal of this project is to predict thermal comfort using MET and HRV, the parameters to be included in the final dataset are:\n",
    "\n",
    "1. index - for practical purposes \n",
    "2. building_id - to separate studies during outlier detection \n",
    "3. ta - temperature \n",
    "4. rh - humidity \n",
    "5. vel - air velocity \n",
    "6. met - due to its relevance for this work \n",
    "7. thermal sensation - the final predicted value  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NaN value rows - maybe reconsider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only the columns considered relevant\n",
    "df_main  = df[['index','building_id','ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "\n",
    "#removing NaN values\n",
    "df_main = df_main.dropna()\n",
    "size_new = df_main['building_id'].size + 1\n",
    "loss = 100 - size_new / size * 100\n",
    "print(loss, size, size_new)\n",
    "\n",
    "#making databases to use for outlier detection\n",
    "df_outliers = df_main[['building_id', 'ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "df_measures = df_main[['ta', 'rh', 'vel', 'met', 'thermal_sensation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23% dataset loss (????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "For the outlier detection different methods are tried below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting outliers for better understanding i think \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sns.boxplot(x=df_outliers['rh'])\n",
    "\n",
    "#judging by the plot, temperature sounds like it shouldn't be getting analyzed that way\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(df_outliers['rh'], df_outliers['building_id'])\n",
    "ax.set_xlabel('humidity')\n",
    "ax.set_ylabel('research')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scores : Describes the point's position relative to the mean of the group (column) in amount of standard deviations - here an outlier would be a point with a z-score of higher than abs(3). The algorithm is not considered as effective since it requires (assumes) a mean. However, potentially useful to know around how many points to expect as outliers during multidimensional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods\n",
    "#z-scores \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from IPython.display import display\n",
    "\n",
    "df_zscore = stats.zscore(df_measures)\n",
    "df_zscore['outlier'] = 0\n",
    "\n",
    "def zfunc(column):\n",
    "    counter = 0\n",
    "    for index, cell in df_zscore[column].items(): \n",
    "        if (not math.isnan(cell)) and (cell>3 or cell<-3):\n",
    "            counter+=1\n",
    "            df_zscore.loc[index, 'outlier']= 1\n",
    "    print ('Number of outliers in column ' + column + ' is: ' + str(counter))\n",
    "    return 0\n",
    "\n",
    "for col in df_measures.columns: \n",
    "    zfunc(col)\n",
    "\n",
    "counter = 0\n",
    "for cell in df_zscore['outlier']: \n",
    "    if cell == 1: counter += 1\n",
    "print('Number of total outliers is: ' + str(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR : Measures the spread of the middle 50% of the data in a column - here an outlier would be a point that would fall out of the 50% range by +/- 1.5 times that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#iqr  \n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "df_iqr = df_measures\n",
    "\n",
    "def iqr_func(column):\n",
    "    q75, q25 = np.percentile(column, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    valid = iqr*2.0\n",
    "    counter = 0\n",
    "    for cell in column:\n",
    "        if  (not math.isnan(cell)) and (cell>q75+valid or cell<q25-valid): \n",
    "            counter+=1\n",
    "\n",
    "    return counter\n",
    "\n",
    "for col in df_iqr.columns: \n",
    "    counter = iqr_func(df_iqr[col])\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation forest : Detects anomalies based on distance taking multiple parameters into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#Isolation tree\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df_iso = df_outliers\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42) #for contamination came to the conclusion using the maximum amount of outliers found in the per column analysis before \n",
    "iso_forest.fit(df_outliers)\n",
    "df_outliers['anomaly'] = iso_forest.predict(df_outliers)\n",
    "\n",
    "counter = 0\n",
    "for index, row in df_iso.iterrows():  \n",
    "        if row['anomaly']==-1: \n",
    "            counter +=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using isolation forest to handle outliers \n",
    "#dropping outliers since it's still kinda unclear what to do\n",
    "#still have to look into it \n",
    "size_before = df_iso['building_id'].size + 1\n",
    "df_iso = df_iso[df_iso['anomaly'] != -1]\n",
    "size_clear = df_iso['building_id'].size + 1\n",
    "print(size_before)\n",
    "print(size_clear)\n",
    "\n",
    "df_final = df_iso\n",
    "size_final = df_final['building_id'].size+1\n",
    "print(size_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive model\n",
    "\n",
    "only the second attempt part SEEMS to work but still with the scaler i feel it's a little confusing whether there's any actual result or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt with ANN - wrong\n",
    "from tensorflow.python.keras.models import Sequential \n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#create data\n",
    "data = df_final \n",
    "X = data[['building_id', 'ta', 'rh', 'vel', 'met']]\n",
    "y = data[['thermal_sensation']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "#separate into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu', input_dim = 5))\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu'))\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu'))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform', activation= 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer= 'adam', loss = 'mse', metrics = ['mse', 'mae'])\n",
    "\n",
    "#fit model\n",
    "model.fit(X_train, y_train, batch_size= 128, epochs= 100)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errors\n",
    "import math\n",
    "rmse = mean_squared_error(y_pred, y_test)\n",
    "print(math.sqrt(rmse))\n",
    "\n",
    "mae = mean_absolute_error(y_pred, y_test)\n",
    "print(mae)\n",
    "\n",
    "test_results = pd.DataFrame(data = {'Predicted':y_pred.ravel(), 'Actual':y_test.ravel()})\n",
    "comparison =  pd.DataFrame(data = {'Original':data['thermal_sensation'], 'New':y.ravel()})\n",
    "path = str(pathlib.Path().resolve()) + '\\\\results.csv' #stores file for future use\n",
    "path2 = str(pathlib.Path().resolve()) + '\\\\comparison.csv' #stores file for future use\n",
    "test_results.to_csv(path, sep = ' ')\n",
    "comparison.to_csv(path2, sep = ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
