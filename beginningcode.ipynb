{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model predicting thermal sensation using given database\n",
    "\n",
    "Link to database: https://github.com/CenterForTheBuiltEnvironment/ashrae-db-II.git\n",
    "\n",
    "Packages: \n",
    "1. pandas\n",
    "2. scipy \n",
    "3. math - no need \n",
    "4. numpy\n",
    "5. scikit\n",
    "6. tensorflow and keras\n",
    "\n",
    "\n",
    "TODO: \n",
    "\n",
    "find correlation between parameters to make future analysis easier\n",
    "\n",
    "instead of sampling without outliers maybe find a way to detect most important records and use those as your sample? \n",
    "\n",
    "use different neural networks to find best results\n",
    "\n",
    "also better analyze the neural network itself to know exactly why you're doing each thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe \n",
    "\n",
    "Using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18120/1151135277.py:5: DtypeWarning: Columns (5,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"db_measurements_v2.1.0.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pathlib\n",
    "\n",
    "#create dataframe from data csv file as df\n",
    "df = pd.read_csv(\"db_measurements_v2.1.0.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values\n",
    "\n",
    "Given the fact that the dataset consists of a collection of different studies, each of which take into consideration varied parameters, the following code calculates the amount of NaN values on each column of the dataframe. The aim here is to find the parameters most commonly used in studies to create a final dataset as consistent as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to find percentage of NaNs per column, types it in txt file\n",
    "\n",
    "#create percentages\n",
    "size = df['index'].size + 1\n",
    "nan_array = df.isnull().sum() / size * 100 #creates a series of the percentages\n",
    "\n",
    "#store in file\n",
    "nan_array_string = [\"%.2f\" % i for i in nan_array] #turns percentages into strings\n",
    "\n",
    "data = {df.columns[col]: nan_array_string[col] for col in range(nan_array.size)} #makes dict and dataframe\n",
    "nan_df = pd.DataFrame(data.items())\n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data.csv' #stores in file\n",
    "nan_df.to_csv(path, header=None, index = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sorting the dataset's columns by their amount of NaN values can allow for an easy selection of columns to keep for the analysis and later prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort through nan series and cut all percentages above 50%\n",
    "\n",
    "nan_array_sorted = nan_array.sort_values(ascending=True) #sorts throught the series \n",
    "nan_array_sorted = nan_array_sorted[nan_array_sorted<50.0] #only keeps columns with below 50% NaN cells \n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data_sorted.csv' #stores file for future use\n",
    "nan_array_sorted.to_csv(path, header = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the file produced and relevant bibliography and keeping in mind that the ultimate goal of this project is to predict thermal comfort using MET and HRV, the parameters to be included in the final dataset are:\n",
    "\n",
    "1. index - for practical purposes \n",
    "2. building_id - to separate studies during outlier detection \n",
    "3. ta - temperature \n",
    "4. rh - humidity \n",
    "5. vel - air velocity \n",
    "6. met - due to its relevance for this work \n",
    "7. thermal sensation - the final predicted value  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NaN value rows - maybe reconsider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.175339802263522 109034 83765\n"
     ]
    }
   ],
   "source": [
    "#keeping only the columns considered relevant\n",
    "df_main  = df[['index','building_id','ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "\n",
    "#removing NaN values\n",
    "df_main = df_main.dropna()\n",
    "size_new = df_main['building_id'].size + 1\n",
    "loss = 100 - size_new / size * 100\n",
    "print(loss, size, size_new)\n",
    "\n",
    "#making databases to use for outlier detection\n",
    "df_outliers = df_main[['building_id', 'ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "df_measures = df_main[['ta', 'rh', 'vel', 'met', 'thermal_sensation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23% dataset loss (????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a correlation matrix between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ta</th>\n",
       "      <th>rh</th>\n",
       "      <th>vel</th>\n",
       "      <th>met</th>\n",
       "      <th>thermal_sensation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ta</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.138527</td>\n",
       "      <td>0.357246</td>\n",
       "      <td>-0.071019</td>\n",
       "      <td>0.360985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rh</th>\n",
       "      <td>0.138527</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>-0.077406</td>\n",
       "      <td>-0.032801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel</th>\n",
       "      <td>0.357246</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029682</td>\n",
       "      <td>0.053853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met</th>\n",
       "      <td>-0.071019</td>\n",
       "      <td>-0.077406</td>\n",
       "      <td>-0.029682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thermal_sensation</th>\n",
       "      <td>0.360985</td>\n",
       "      <td>-0.032801</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>0.057938</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ta        rh       vel       met  thermal_sensation\n",
       "ta                 1.000000  0.138527  0.357246 -0.071019           0.360985\n",
       "rh                 0.138527  1.000000  0.204945 -0.077406          -0.032801\n",
       "vel                0.357246  0.204945  1.000000 -0.029682           0.053853\n",
       "met               -0.071019 -0.077406 -0.029682  1.000000           0.057938\n",
       "thermal_sensation  0.360985 -0.032801  0.053853  0.057938           1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_measures.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be little to no correlation between the prediction goal feature and those used in this dataset. That said, correlation matrix only accounts for individual features and not their combined outcome. Given the fact that temperature appears to somehow affect thermal sensation to a degree, plus the importance of features like MET for the final result of this research, features are to be kept that way. Thermal preferance and acceptability will also be considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "For the outlier detection different methods are tried below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting outliers for better understanding i think \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sns.boxplot(x=df_outliers['rh'])\n",
    "\n",
    "#judging by the plot, temperature sounds like it shouldn't be getting analyzed that way\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(df_outliers['rh'], df_outliers['building_id'])\n",
    "ax.set_xlabel('humidity')\n",
    "ax.set_ylabel('research')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scores : Describes the point's position relative to the mean of the group (column) in amount of standard deviations - here an outlier would be a point with a z-score of higher than abs(3). The algorithm is not considered as effective since it requires (assumes) a mean. However, potentially useful to know around how many points to expect as outliers during multidimensional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods\n",
    "#z-scores \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from IPython.display import display\n",
    "\n",
    "df_zscore = stats.zscore(df_measures.copy())\n",
    "df_zscore['outlier'] = 0\n",
    "\n",
    "\n",
    "def zfunc(column):\n",
    "    counter = 0\n",
    "    for index, cell in df_zscore[column].items(): \n",
    "        if (not math.isnan(cell)) and (cell>3 or cell<-3):\n",
    "            counter+=1\n",
    "            df_zscore.loc[index, 'outlier']= 1\n",
    "    print ('Number of outliers in column ' + column + ' is: ' + str(counter))\n",
    "    return 0\n",
    "\n",
    "for col in df_measures.columns: \n",
    "    zfunc(col)\n",
    "\n",
    "counter = 0\n",
    "for cell in df_zscore['outlier']: \n",
    "    if cell == 1: counter += 1\n",
    "print('Number of total outliers is: ' + str(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR : Measures the spread of the middle 50% of the data in a column - here an outlier would be a point that would fall out of the 50% range by +/- 1.5 times that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#iqr  \n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "df_iqr = df_measures.copy()\n",
    "df_iqr['outlier'] = 0 \n",
    "\n",
    "def iqr_func(column):\n",
    "    q75, q25 = np.percentile(df_iqr[column], [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    valid = iqr*2.5 \n",
    "    counter = 0\n",
    "    for index, cell in df_iqr[column].items():\n",
    "        if  (not math.isnan(cell)) and (cell>q75+valid or cell<q25-valid): \n",
    "            df_iqr.loc[index, 'outlier'] = 1\n",
    "            counter+=1\n",
    "    print ('Number of outliers in column ' + column + \" is: \" + str(counter))\n",
    "    return 0\n",
    "\n",
    "for col in df_measures.columns: \n",
    "    iqr_func(col)\n",
    "\n",
    "counter = 0\n",
    "for cell in df_iqr['outlier']: \n",
    "    if cell ==1: counter += 1\n",
    "print('Number of total outliers is: ' + str(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation forest : Detects anomalies based on distance taking multiple parameters into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#Isolation tree\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df_iso = df_outliers\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.08, random_state=42) #for contamination came to the conclusion using the maximum amount of outliers found in the per column analysis before \n",
    "iso_forest.fit(df_outliers)\n",
    "df_outliers['anomaly'] = iso_forest.predict(df_outliers)\n",
    "\n",
    "counter = 0\n",
    "for index, row in df_iso.iterrows():  \n",
    "        if row['anomaly']==-1: \n",
    "            counter +=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using isolation forest to handle outliers \n",
    "#dropping outliers since it's still kinda unclear what to do\n",
    "#still have to look into it \n",
    "size_before = df_iso['building_id'].size + 1\n",
    "df_iso = df_iso[df_iso['anomaly'] != -1]\n",
    "size_clear = df_iso['building_id'].size + 1\n",
    "print(size_before)\n",
    "print(size_clear)\n",
    "\n",
    "df_final = df_iso\n",
    "size_final = df_final['building_id'].size+1\n",
    "print(size_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN: clustering algorithm that calculates distance between points and clusters them given a minimum amount of points to be included in each cluster and the radius of the cluster (minPoints and epsilon). \n",
    "For the minPoints parameter, it was chosen here to be double the amount of dimensions in the database. For the epsilon parameter, a k-distance graph is created using euclidean distance out of which, epsilon is the range in which the most k-distances are gathered. \n",
    "\n",
    "Another distance measure to use is Mahalanobis, that takes into consideration the multidimensionality of the database for more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a k-distance graph \n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df_scan = df_measures.copy()\n",
    "scan_array = df_scan.to_numpy()\n",
    "minPoints = 10\n",
    "\n",
    "#making a randomized database with 1000 points (check on the accuracy of analysis with that sample plus error margin etc)\n",
    "def make_pop(og_array): \n",
    "    new_array = []\n",
    "    for i in range (1000): \n",
    "        ind = random.randint(0, len(og_array)-1)\n",
    "        new_array.append(og_array[ind])\n",
    "\n",
    "    return new_array \n",
    "\n",
    "#currently using euclidean distance, will turn to mahalanobis\n",
    "def dist(p1, p2): \n",
    "    dist = 0\n",
    "    for i in range(len(p1)):\n",
    "        dist = dist + (p1[i] - p2[i])**2\n",
    "    return math.sqrt(dist)\n",
    "\n",
    "#making table to store all distances complexity is O(5n^2) plus it takes up a lot of storage too so should probably be deleted at some point\n",
    "def dist_table(ar): \n",
    "    tb = np.zeros((len(ar), len(ar)))\n",
    "    for i in range(len(ar)): \n",
    "        for j in range(i, len(ar)): \n",
    "            tb[i][j] = dist(ar[i], ar[j])\n",
    "        for j in range(0, i): \n",
    "            tb[i][j] = tb[j][i]\n",
    "    return tb\n",
    "\n",
    "sample_array = make_pop(scan_array)\n",
    "dtable = dist_table(sample_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's where I'll find the actual epsilon and do the BDSCAN\n",
    "\n",
    "#findind k-NN so as to determine a good distance to use as epsilon\n",
    "def k_NN_func(dist_table, k):\n",
    "    k_NN_dist = []\n",
    "    eucl_dist = []\n",
    "    for i in range(len(dist_table)):\n",
    "        eucl_dist = dist_table[i]\n",
    "        eucl_dist.sort()\n",
    "        k_NN_dist.append(eucl_dist[k])\n",
    "\n",
    "    return k_NN_dist\n",
    "\n",
    "distances = k_NN_func(dtable, 10)\n",
    "\n",
    "plt.hist(distances,bins=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon appears to be around 10.5 here and that's the number that's to be used for the DBSCAN with minPoints being 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the DBSCAN \n",
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "dbscan = DBSCAN(eps=10.5, min_samples=10).fit(df_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive model\n",
    "\n",
    "only the second attempt part SEEMS to work but still with the scaler i feel it's a little confusing whether there's any actual result or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt with ANN - wrong\n",
    "from tensorflow.python.keras.models import Sequential \n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#create data\n",
    "data = df_final \n",
    "X = data[['building_id', 'ta', 'rh', 'vel', 'met']]\n",
    "y = data[['thermal_sensation']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "#separate into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu', input_dim = 5))\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu'))\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu'))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform', activation= 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer= 'adam', loss = 'mse', metrics = ['mse', 'mae'])\n",
    "\n",
    "#fit model\n",
    "model.fit(X_train, y_train, batch_size= 128, epochs= 100)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errors\n",
    "import math\n",
    "rmse = mean_squared_error(y_pred, y_test)\n",
    "print(math.sqrt(rmse))\n",
    "\n",
    "mae = mean_absolute_error(y_pred, y_test)\n",
    "print(mae)\n",
    "\n",
    "test_results = pd.DataFrame(data = {'Predicted':y_pred.ravel(), 'Actual':y_test.ravel()})\n",
    "comparison =  pd.DataFrame(data = {'Original':data['thermal_sensation'], 'New':y.ravel()})\n",
    "path = str(pathlib.Path().resolve()) + '\\\\results.csv' #stores file for future use\n",
    "path2 = str(pathlib.Path().resolve()) + '\\\\comparison.csv' #stores file for future use\n",
    "test_results.to_csv(path, sep = ' ')\n",
    "comparison.to_csv(path2, sep = ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
