{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model predicting thermal sensation using given database\n",
    "\n",
    "Link to database: https://github.com/CenterForTheBuiltEnvironment/ashrae-db-II.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe \n",
    "\n",
    "Using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MartyPickles\\AppData\\Local\\Temp\\ipykernel_19424\\1151135277.py:5: DtypeWarning: Columns (5,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"db_measurements_v2.1.0.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pathlib\n",
    "\n",
    "#create dataframe from data csv file as df\n",
    "df = pd.read_csv(\"db_measurements_v2.1.0.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values\n",
    "\n",
    "Given the fact that the dataset consists of a collection of different studies, each of which take into consideration varied parameters, the following code calculates the amount of NaN values on each column of the dataframe. The aim here is to find the most common parameters used among the studies to create a final dataframe as consistent as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to find percentage of NaNs per column, types it in txt file\n",
    "\n",
    "#create percentages\n",
    "size = df['index'].size + 1\n",
    "nan_array = df.isnull().sum() / size * 100 #creates a series of the percentages\n",
    "\n",
    "#store in file\n",
    "nan_array_string = [\"%.2f\" % i for i in nan_array] #turns percentages into strings\n",
    "\n",
    "data = {df.columns[col]: nan_array_string[col] for col in range(nan_array.size)} #makes dict and dataframe\n",
    "nan_df = pd.DataFrame(data.items())\n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data.txt' #stores in file\n",
    "nan_df.to_csv(path, header=None, index = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sorting the dataset's columns by their amount of NaN values can allow for an easy selection of columns to keep for the analysis and later prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort through nan series and cut all percentages above 50%\n",
    "\n",
    "nan_array_sorted = nan_array.sort_values(ascending=True) #sorts throught the series \n",
    "nan_array_sorted = nan_array_sorted[nan_array_sorted<50.0] #only keeps columns with below 50% NaN cells \n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data_sorted.txt' #stores file for future use\n",
    "nan_array_sorted.to_csv(path, header = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the file produced and relevant bibliography and keeping in mind that the ultimate goal of this project is to predict thermal comfort using MET and HRV, the parameters to be included in the final dataset are:\n",
    "\n",
    "1. index - for practical purposes \n",
    "2. building_id - to separate studies during outlier detection \n",
    "3. ta - temperature \n",
    "4. rh - humidity \n",
    "5. vel - air velocity \n",
    "6. met - due to its relevance for this work \n",
    "7. thermal sensation - the final predicted value  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding NaN values, since the data comes from different studies and thus they can not simply be adjusted to comform to a general tendency, it was decided that the rows including them be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.175339802263522\n"
     ]
    }
   ],
   "source": [
    "#keeping only a few of the columns for the test in df_outliers dataframe\n",
    "df_outliers  = df[['index','building_id','ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "\n",
    "#removing NaN values\n",
    "df_outliers = df_outliers.dropna()\n",
    "size_new = df_outliers['index'].size + 1\n",
    "loss = 100 - size_new / size * 100\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this, by removing NaN values, the loss is about 23% of the database, a relatively acceptable number (I think?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "For the outlier detection different methods are tried below. \n",
    "\n",
    "*WOULD BE NICE IF I COULD ACTUALLY PLOT DATA BUT WOULDN'T YOU KNOW IT, PIL ISN'T WORKING NOW? NO FINAL DECISION MADE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scores : Using the variance from each value by a mean, when applied to each of the study parameters, this technique detects the most variant values. \n",
    "It is considered not as effective since it requires a mean to exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods\n",
    "#z-scores \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "df_zscore = stats.zscore(df_outliers, nan_policy = 'omit')\n",
    "\n",
    "def zfunc(column):\n",
    "    counter = 0\n",
    "    for cell in df_zscore[column]: \n",
    "        if (not math.isnan(cell)) and (cell>3 or cell<-3):\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "for col in df_zscore.columns:\n",
    "    counter = zfunc(col)\n",
    "    print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR : Removes the values that are higher than the 75th and lower than the 25th percentile of the same column by some multiple of the range among them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#iqr  \n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "df_iqr = df_outliers\n",
    "\n",
    "def iqr_func(column):\n",
    "    q75, q25 = np.percentile(column, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    valid = iqr*2.0\n",
    "    counter = 0\n",
    "    for cell in column:\n",
    "        if  (not math.isnan(cell)) and (cell>q75+valid or cell<q25-valid): \n",
    "            counter+=1\n",
    "\n",
    "    return counter\n",
    "\n",
    "for col in df_iqr.columns: \n",
    "    counter = iqr_func(df_iqr[col])\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation forest : Algorithm to detect anomalies based on distance from other datapoints. Considered best here since it takes multiple parameters into consideration at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#Isolation tree\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df_iso = df_outliers\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "iso_forest.fit(df_outliers)\n",
    "df_outliers['anomaly'] = iso_forest.predict(df_outliers)\n",
    "\n",
    "counter = 0\n",
    "for index, row in df_iso.iterrows():  \n",
    "        if row['anomaly']==-1: \n",
    "            counter +=1\n",
    "print(counter)\n",
    "\n",
    "#since python has decided not to work with PIL and thus I can't plot anything\n",
    "#i am now deciding that this is the best practice to remove outliers until I can \n",
    "#solve the issue since I've spent too much time on outliers and no results have \n",
    "#come forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83765\n",
      "75388\n",
      "75388\n"
     ]
    }
   ],
   "source": [
    "#using isolation forest to handle outliers \n",
    "#dropping outliers since it's still kinda unclear what to do\n",
    "#still have to look into it \n",
    "size_before = df_iso['index'].size + 1\n",
    "df_iso = df_iso[df_iso['anomaly'] != -1]\n",
    "size_clear = df_iso['index'].size + 1\n",
    "print(size_before)\n",
    "print(size_clear)\n",
    "\n",
    "df_final = df_iso\n",
    "size_final = df_final['index'].size+1\n",
    "print(size_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not a timeseries so I think i'm using regression\n",
    "#to do that I first need to find out how to implement the linear regression\n",
    "#there's steps to follow namely: \n",
    "#1. separate into training test and evaluation - can't i easily fit that with a package\n",
    "#2. install tensorflow and keras probably - done \n",
    "#3. make python work? somehow - done \n",
    "#4. run the training and the testing, evaluate and somehow present results \n",
    "#MAYBE this can be done in a night but I don't think that's the best route to follow\n",
    "#regardless, tomorrow you gotta call the transfer company and also take pictures \n",
    "#of your furniture to sell \n",
    "#and then sleep\n",
    "#do those and then you just got the cvs to send and react with backend to learn\n",
    "#which will take around the same amount of time as both getting pregnant and giving\n",
    "#birth \n",
    "#in the meantime have fun looking for a server job once you get to athens and slowly\n",
    "#drifting away from uni and all the work you've done or the possibility \n",
    "#of ever getting a fulfilling job out of the service industry! \n",
    "#peace and love! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE: i've been waiting for tensorflow and keras to be installed for like 15 minutes \n",
    "#now and I don't even know that they will actually work\n",
    "#funny thing is i'm quite sure they won't since they never do unless you spend \n",
    "#a few hours looking into their issues that will end up being something stupid \n",
    "#you couldn't have predicted \n",
    "#thanks stack overflow? \n",
    "\n",
    "#there, they were already found incompatible with that version of python just like the rest of the things I've \n",
    "#tried to use, wonderful init?\n",
    "\n",
    "#SECOND UPDATE: python has fucked me up, don't use 3.11 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
