{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to be followed:\n",
    "\n",
    "1. calculate missing data per column to figure if it can be used - done\n",
    "2. according to bibliography and columns, figure out those to remain - done\n",
    "3. outliers? - done\n",
    "4. convert everything into numerical and normalized if needs be\n",
    "5. separate into training, test, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pathlib\n",
    "\n",
    "#create dataframe from data csv file as df\n",
    "df = pd.read_csv(\"db_measurements_v2.1.0.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out which columns to use, the first step would be to look for a percentage of NaN values on each. Seeing as the dataset is made up of a collection of studies, each of which have different parameters, maintaining those columns that are common among them is the easiest way to ensure some consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to find percentage of NaNs per column, types it in txt file\n",
    "\n",
    "#create percentages\n",
    "size = df['index'].size + 1\n",
    "nan_array = df.isnull().sum() / size * 100 #creates a series of the percentages\n",
    "\n",
    "#store in file\n",
    "nan_array_string = [\"%.2f\" % i for i in nan_array] #turns percentages into strings\n",
    "\n",
    "data = {df.columns[col]: nan_array_string[col] for col in range(nan_array.size)} #makes dict and dataframe\n",
    "nan_df = pd.DataFrame(data.items())\n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data.txt' #stores in file\n",
    "nan_df.to_csv(path, header=None, index = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sorting the dataset's columns by their amount of NaN values can allow for an easy selection of columns to keep for the analysis and later prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort through nan series and cut all percentages above 50%\n",
    "\n",
    "nan_array_sorted = nan_array.sort_values(ascending=True) #sorts throught the series \n",
    "nan_array_sorted = nan_array_sorted[nan_array_sorted<50.0] #only keeps columns with below 50% NaN cells \n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data_sorted.txt' #stores file for future use\n",
    "nan_array_sorted.to_csv(path, header = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the file produced and relevant bibliography and keeping in mind that the ultimate goal of this project is to predict thermal comfort using MET and HRV, the parameters to be included in the final dataset are:\n",
    "\n",
    "1. index - for practical purposes \n",
    "2. building_id - to separate studies during outlier detection \n",
    "3. ta - temperature \n",
    "4. rh - humidity \n",
    "5. vel - air velocity \n",
    "6. met - due to its relevance for this work \n",
    "7. thermal sensation - the final predicted value  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding NaN values, since the data comes from different studies and thus they can not simply be adjusted to comform to a general tendency, it was decided that the rows including them be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only a few of the columns for the test in df_outliers dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "df_outliers  = df[['index','building_id','ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "#TODO: should I keep building_id?\n",
    "\n",
    "#removing NaN values\n",
    "df_outliers = df_outliers.dropna()\n",
    "size_new = df_outliers['index'].size + 1\n",
    "loss = 100 - size_new / size * 100\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this, by removing NaN values, the loss is about 23% of the database, a relatively acceptable number (I think?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the outlier prediction, three different methods are used below. \n",
    "\n",
    "Z-scores : Using the variance from each value by a mean, when applied to each of the study parameters, this technique detects the most variant values. \n",
    "It is considered not as effective since it requires a mean to exist. \n",
    "\n",
    "IQR : Removes the values that are higher than the 75th and lower than the 25th percentile of the same column by some multiple of the range among them. \n",
    "\n",
    "Isolation forest : Algorithm to detect anomalies based on distance from other datapoints. Considered best here since it takes multiple parameters into consideration at once. \n",
    "\n",
    "WOULD BE NICE IF I COULD ACTUALLY PLOT DATA BUT WOULDN'T YOU KNOW IT, PIL ISN'T WORKING NOW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods\n",
    "#z-scores - using scipy, applying for each column and finding 3 or more deviation cells\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "df_zscore = stats.zscore(df_outliers, nan_policy = 'omit')\n",
    "\n",
    "def zfunc(column):\n",
    "    counter = 0\n",
    "    for cell in df_zscore[column]: \n",
    "        if (not math.isnan(cell)) and (cell>3 or cell<-3):\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "for col in df_zscore.columns:\n",
    "    counter = zfunc(col)\n",
    "    print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#iqr - again just checking, going to fix code here \n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "df_iqr = df_outliers\n",
    "\n",
    "def iqr_func(column):\n",
    "    q75, q25 = np.percentile(column, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    valid = iqr*2.0\n",
    "    counter = 0\n",
    "    for cell in column:\n",
    "        if  (not math.isnan(cell)) and (cell>q75+valid or cell<q25-valid): \n",
    "            counter+=1\n",
    "\n",
    "    return counter\n",
    "\n",
    "for col in df_iqr.columns: \n",
    "    counter = iqr_func(df_iqr[col])\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#Isolation tree\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df_iso = df_outliers\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "iso_forest.fit(df_outliers)\n",
    "df_outliers['anomaly'] = iso_forest.predict(df_outliers)\n",
    "\n",
    "counter = 0\n",
    "for index, row in df_iso.iterrows():  \n",
    "        if row['anomaly']==-1: \n",
    "            counter +=1\n",
    "print(counter)\n",
    "\n",
    "#since python has decided not to work with PIL and thus I can't plot anything\n",
    "#i am now deciding that this is the best practice to remove outliers until I can \n",
    "#solve the issue since I've spent too much time on outliers and no results have \n",
    "#come forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out if I need to normalize I need to know what model I will be using. That said, it's probably useful to leave space for this step and then see what to do. So that's a reminder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
