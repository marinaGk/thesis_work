{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model predicting thermal sensation using given database\n",
    "\n",
    "Link to database: https://github.com/CenterForTheBuiltEnvironment/ashrae-db-II.git\n",
    "\n",
    "Packages: \n",
    "1. pandas\n",
    "2. scipy \n",
    "3. math - no need \n",
    "4. numpy\n",
    "5. scikit\n",
    "6. tensorflow and keras\n",
    "\n",
    "### So far: \n",
    "\n",
    "1. To begin with preprocessing is rushed to say the least. \n",
    "2. Parameters are picked among those with the least NaN values but instead of sampling, it gets rid of all NaN rows. \n",
    "3. Then, for the outlier detection, there's another amount of rows dropped, no reasoning there. Employs standard scaler later on without checking other methods. \n",
    "4. Main model is an ANN regressor that however fails entirely seeing as the mse and mae cannot really show anything when data have been scaled that way.\n",
    "5. Still haven't plotted outliers out because old env would not work with pillow. \n",
    "6. Look into what kfold does since you clearly don't remember. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe \n",
    "\n",
    "Using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MartyPickles\\AppData\\Local\\Temp\\ipykernel_2276\\1151135277.py:5: DtypeWarning: Columns (5,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"db_measurements_v2.1.0.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pathlib\n",
    "\n",
    "#create dataframe from data csv file as df\n",
    "df = pd.read_csv(\"db_measurements_v2.1.0.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values\n",
    "\n",
    "Given the fact that the dataset consists of a collection of different studies, each of which take into consideration varied parameters, the following code calculates the amount of NaN values on each column of the dataframe. The aim here is to find the most common parameters used among the studies to create a final dataframe as consistent as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to find percentage of NaNs per column, types it in txt file\n",
    "\n",
    "#create percentages\n",
    "size = df['index'].size + 1\n",
    "nan_array = df.isnull().sum() / size * 100 #creates a series of the percentages\n",
    "\n",
    "#store in file\n",
    "nan_array_string = [\"%.2f\" % i for i in nan_array] #turns percentages into strings\n",
    "\n",
    "data = {df.columns[col]: nan_array_string[col] for col in range(nan_array.size)} #makes dict and dataframe\n",
    "nan_df = pd.DataFrame(data.items())\n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data.csv' #stores in file\n",
    "nan_df.to_csv(path, header=None, index = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sorting the dataset's columns by their amount of NaN values can allow for an easy selection of columns to keep for the analysis and later prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort through nan series and cut all percentages above 50%\n",
    "\n",
    "nan_array_sorted = nan_array.sort_values(ascending=True) #sorts throught the series \n",
    "nan_array_sorted = nan_array_sorted[nan_array_sorted<50.0] #only keeps columns with below 50% NaN cells \n",
    "\n",
    "path = str(pathlib.Path().resolve()) + '\\data_sorted.csv' #stores file for future use\n",
    "nan_array_sorted.to_csv(path, header = None, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the file produced and relevant bibliography and keeping in mind that the ultimate goal of this project is to predict thermal comfort using MET and HRV, the parameters to be included in the final dataset are:\n",
    "\n",
    "1. index - for practical purposes \n",
    "2. building_id - to separate studies during outlier detection \n",
    "3. ta - temperature \n",
    "4. rh - humidity \n",
    "5. vel - air velocity \n",
    "6. met - due to its relevance for this work \n",
    "7. thermal sensation - the final predicted value  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding NaN values, since the data comes from different studies and thus they can not simply be adjusted to comform to a general tendency, it was decided that the rows including them be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only a few of the columns for the test in df_outliers dataframe\n",
    "df_outliers  = df[['index','building_id','ta', 'rh', 'vel', 'met', 'thermal_sensation']]\n",
    "\n",
    "#removing NaN values\n",
    "df_outliers = df_outliers.dropna()\n",
    "size_new = df_outliers['index'].size + 1\n",
    "loss = 100 - size_new / size * 100\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this, by removing NaN values, the loss is about 23% of the database, a relatively acceptable number (I think?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "For the outlier detection different methods are tried below. \n",
    "\n",
    "*WOULD BE NICE IF I COULD ACTUALLY PLOT DATA BUT WOULDN'T YOU KNOW IT, PIL ISN'T WORKING NOW? NO FINAL DECISION MADE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scores : Using the variance from each value by a mean, when applied to each of the study parameters, this technique detects the most variant values. \n",
    "It is considered not as effective since it requires a mean to exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods\n",
    "#z-scores \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "df_zscore = stats.zscore(df_outliers, nan_policy = 'omit')\n",
    "\n",
    "def zfunc(column):\n",
    "    counter = 0\n",
    "    for cell in df_zscore[column]: \n",
    "        if (not math.isnan(cell)) and (cell>3 or cell<-3):\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "for col in df_zscore.columns:\n",
    "    counter = zfunc(col)\n",
    "    print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR : Removes the values that are higher than the 75th and lower than the 25th percentile of the same column by some multiple of the range among them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#iqr  \n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "df_iqr = df_outliers\n",
    "\n",
    "def iqr_func(column):\n",
    "    q75, q25 = np.percentile(column, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    valid = iqr*2.0\n",
    "    counter = 0\n",
    "    for cell in column:\n",
    "        if  (not math.isnan(cell)) and (cell>q75+valid or cell<q25-valid): \n",
    "            counter+=1\n",
    "\n",
    "    return counter\n",
    "\n",
    "for col in df_iqr.columns: \n",
    "    counter = iqr_func(df_iqr[col])\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation forest : Algorithm to detect anomalies based on distance from other datapoints. Considered best here since it takes multiple parameters into consideration at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different outlier methods \n",
    "#Isolation tree\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df_iso = df_outliers\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "iso_forest.fit(df_outliers)\n",
    "df_outliers['anomaly'] = iso_forest.predict(df_outliers)\n",
    "\n",
    "counter = 0\n",
    "for index, row in df_iso.iterrows():  \n",
    "        if row['anomaly']==-1: \n",
    "            counter +=1\n",
    "print(counter)\n",
    "\n",
    "#since python has decided not to work with PIL and thus I can't plot anything\n",
    "#i am now deciding that this is the best practice to remove outliers until I can \n",
    "#solve the issue since I've spent too much time on outliers and no results have \n",
    "#come forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using isolation forest to handle outliers \n",
    "#dropping outliers since it's still kinda unclear what to do\n",
    "#still have to look into it \n",
    "size_before = df_iso['index'].size + 1\n",
    "df_iso = df_iso[df_iso['anomaly'] != -1]\n",
    "size_clear = df_iso['index'].size + 1\n",
    "print(size_before)\n",
    "print(size_clear)\n",
    "\n",
    "df_final = df_iso\n",
    "size_final = df_final['index'].size+1\n",
    "print(size_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive model\n",
    "\n",
    "only the second attempt part SEEMS to work but still with the scaler i feel it's a little confusing whether there's any actual result or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt with ANN - wrong\n",
    "from tensorflow.python.keras.models import Sequential \n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#create data\n",
    "data = df_final \n",
    "X = data[['index', 'building_id', 'ta', 'rh', 'vel', 'met']]\n",
    "y = data[['thermal_sensation']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "#separate into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu', input_dim = 6))\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu'))\n",
    "model.add(Dense(40, kernel_initializer= 'uniform', activation= 'relu'))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform', activation= 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer= 'adam', loss = 'mse', metrics = ['mse', 'mae'])\n",
    "\n",
    "#fit model\n",
    "model.fit(X_train, y_train, batch_size= 128, epochs= 100)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errors\n",
    "import math\n",
    "rmse = mean_squared_error(y_pred, y_test)\n",
    "print(math.sqrt(rmse))\n",
    "\n",
    "mae = mean_absolute_error(y_pred, y_test)\n",
    "print(mae)\n",
    "\n",
    "test_results = pd.DataFrame(data = {'Predicted':y_pred.ravel(), 'Actual':y_test.ravel()})\n",
    "comparison =  pd.DataFrame(data = {'Original':data['thermal_sensation'], 'New':y.ravel()})\n",
    "path = str(pathlib.Path().resolve()) + '\\\\results.csv' #stores file for future use\n",
    "path2 = str(pathlib.Path().resolve()) + '\\\\comparison.csv' #stores file for future use\n",
    "test_results.to_csv(path, sep = ' ')\n",
    "comparison.to_csv(path2, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first attempt with linear regression - doesn't work, is missing some code?\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential \n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "data = df_final \n",
    "\n",
    "X = data[['index', 'building_id', 'ta', 'rh', 'vel', 'met']]\n",
    "y = data['thermal_sensation']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "def baseline_model(): \n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_shape= (6,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50, batch_size=5, verbose=1)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X_train, Y_train, cv=kfold ,scoring= 'neg_mean_squared_error')\n",
    "\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest - wrong also missing code\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df_final\n",
    "\n",
    "X=data[['index', 'building_id', 'ta', 'rh', 'vel', 'met']]\n",
    "y=data[['thermal_sensation']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "n_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
